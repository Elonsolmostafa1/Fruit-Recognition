{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc27721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn import svm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from skimage.filters import threshold_yen\n",
    "from skimage.exposure import rescale_intensity\n",
    "from scipy.stats import skew\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler as sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117b05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apple_training_data='dataset\\\\Apples\\\\Train\\\\*.jpg'\n",
    "Orange_training_data='dataset\\\\Oranges\\\\Train\\\\*.jpg'\n",
    "Mango_training_data='dataset\\\\Mangoes\\\\Train\\\\*.jpg'\n",
    "Test_data='dataset\\\\test\\\\*.jpg'\n",
    "\n",
    "Apple_hue_arr = []   #Info of hue of  each apple pic\n",
    "Orange_hue_arr = []  #Info of hue of  each orange pic\n",
    "Mango_hue_arr = []   #Info of hue of  each mango pic\n",
    "Apple_des_list = []\n",
    "Orange_des_list = []\n",
    "Mango_des_list = []\n",
    "sift = cv2.SIFT_create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9136bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(Apple_training_data):    \n",
    "    img = cv2.imread(file)  #read apple images\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)  #convert from RBG to HSV\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kpts, des = sift.detectAndCompute(img_gray,None)\n",
    "    if len(kpts) < 1:\n",
    "        no_kpts = np.zeros((1, sift.descriptorSize()), np.float32)\n",
    "        Apple_des_list.append((file, no_kpts))\n",
    "    else:\n",
    "        Apple_des_list.append((file, des)) \n",
    "    h,s,v=cv2.split(img)  #Split the the channels of each image\n",
    "    hue_mean=np.mean(h)  #calculate the mean of hue channel of each image\n",
    "    hue_var=np.var(h)    #calculate the var of hue channel of each image\n",
    "    flatten_h = list(np.concatenate(h).flat)  #convert the 2-D image to 1-D image\n",
    "    flatten_s = list(np.concatenate(s).flat)\n",
    "    hue_skewness=skew(flatten_h)   #calculate the skewness of hue channel of each image\n",
    "    image_hue_info=[]\n",
    "    image_hue_info.append(hue_mean)\n",
    "    image_hue_info.append(hue_var)\n",
    "    image_hue_info.append(hue_skewness)\n",
    "    Apple_hue_arr.append(image_hue_info)\n",
    "print(len(Apple_hue_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f10fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the descriptors vertically in a numpy array\n",
    "descriptors = Apple_des_list[0][1]\n",
    "for file, descriptor in Apple_des_list[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6613cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = descriptors.astype(float)\n",
    "\n",
    "k = 60  #k means awal 60 clusters\n",
    "voc, variance = kmeans(descriptors_float, k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f14c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apple_features = np.zeros((72, k), \"float32\")\n",
    "for i in range(72):\n",
    "    words, distance = vq(Apple_des_list[i][1],voc)\n",
    "    for w in words:\n",
    "        Apple_features[i][w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "791efcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apples_all_features=np.concatenate((Apple_hue_arr,Apple_features.tolist()),axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419b11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(Orange_training_data):    \n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kpts, des = sift.detectAndCompute(img_gray,None)\n",
    "    if len(kpts) < 1:\n",
    "        no_kpts = np.zeros((1, sift.descriptorSize()), np.float32)\n",
    "        Orange_des_list.append((file, no_kpts))\n",
    "    else:\n",
    "        Orange_des_list.append((file, des)) \n",
    "    h,s,v=cv2.split(img)\n",
    "    hue_mean=np.mean(h)  #calculate the mean of hue channel of each image\n",
    "    hue_var=np.var(h) \n",
    "    flatten_h = list(np.concatenate(h).flat)\n",
    "    hue_skewness=skew(flatten_h)\n",
    "    image_hue_info=[]\n",
    "    image_hue_info.append(hue_mean)\n",
    "    image_hue_info.append(hue_var)\n",
    "    image_hue_info.append(hue_skewness)\n",
    "    Orange_hue_arr.append(image_hue_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491e337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the descriptors vertically in a numpy array\n",
    "descriptors = Orange_des_list[0][1]\n",
    "for file, descriptor in Orange_des_list[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e963b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = descriptors.astype(float)\n",
    "\n",
    "k = 60  #k means awal 60 clusters\n",
    "voc, variance = kmeans(descriptors_float, k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3dc6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "Orange_features = np.zeros((70, k), \"float32\")\n",
    "for i in range(70):\n",
    "    words, distance = vq(Orange_des_list[i][1],voc)\n",
    "    for w in words:\n",
    "        Orange_features[i][w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e56a7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "Oranges_all_features=np.concatenate((Orange_hue_arr,Orange_features.tolist()),axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12fba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(Mango_training_data):    \n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kpts, des = sift.detectAndCompute(img_gray,None)\n",
    "    if len(kpts) < 1:\n",
    "        no_kpts = np.zeros((1, sift.descriptorSize()), np.float32)\n",
    "        Mango_des_list.append((file, no_kpts))\n",
    "    else:\n",
    "        Mango_des_list.append((file, des)) \n",
    "    h,s,v=cv2.split(img)\n",
    "    hue_mean=np.mean(h)\n",
    "    hue_var=np.var(h)\n",
    "    flatten_h = list(np.concatenate(h).flat)\n",
    "    hue_skewness=skew(flatten_h)\n",
    "    image_hue_info=[]\n",
    "    image_hue_info.append(hue_mean)\n",
    "    image_hue_info.append(hue_var)\n",
    "    image_hue_info.append(hue_skewness)\n",
    "    Mango_hue_arr.append(image_hue_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3551c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the descriptors vertically in a numpy array\n",
    "descriptors = Mango_des_list[0][1]\n",
    "for file, descriptor in Mango_des_list[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d45309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = descriptors.astype(float)\n",
    "\n",
    "k = 60  #k means awal 60 clusters\n",
    "voc, variance = kmeans(descriptors_float, k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bddbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Mango_features = np.zeros((78, k), \"float32\")\n",
    "for i in range(78):\n",
    "    words, distance = vq(Mango_des_list[i][1],voc)\n",
    "    for w in words:\n",
    "        Mango_features[i][w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ae3a5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mangos_all_features=np.concatenate((Mango_hue_arr,Mango_features.tolist()),axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "98f14307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "['Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango']\n",
      "220\n",
      "[48.96833125, 1190.4076741736064, -0.267091441509736, 79.0, 119.0, 189.0, 142.0, 146.0, 138.0, 81.0, 125.0, 96.0, 129.0, 71.0, 99.0, 96.0, 134.0, 202.0, 36.0, 160.0, 99.0, 122.0, 152.0, 88.0, 83.0, 180.0, 22.0, 137.0, 151.0, 85.0, 113.0, 144.0, 166.0, 111.0, 72.0, 89.0, 117.0, 128.0, 90.0, 98.0, 43.0, 98.0, 201.0, 53.0, 145.0, 133.0, 134.0, 182.0, 176.0, 19.0, 184.0, 98.0, 96.0, 114.0, 127.0, 176.0, 105.0, 98.0, 65.0, 116.0, 91.0, 106.0, 38.0]\n"
     ]
    }
   ],
   "source": [
    "#print(len(Apple_hue_arr),len(Orange_hue_arr),len(Mango_hue_arr))\n",
    "a=[\"Apple\"]*len(Apples_all_features)\n",
    "o=[\"Orange\"]*len(Oranges_all_features)\n",
    "m=[\"Mango\"]*len(Mangos_all_features)\n",
    "all_labels=a+o+m\n",
    "print(len(all_labels))\n",
    "print(all_labels)\n",
    "\n",
    "all_features= Apples_all_features + Oranges_all_features + Mangos_all_features\n",
    "print(len(all_features))\n",
    "print(all_features[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff05e0",
   "metadata": {},
   "source": [
    "#pca\n",
    "pca = PCA()\n",
    "x_scaled_pca = pca.fit_transform(all_features)\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_*100,decimals=1)\n",
    "labels =[str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "\n",
    "plt.bar(x=range(1, len(per_var)+1), height=per_var)\n",
    "plt.tick_params(\n",
    "    axis='x',\n",
    "    which='both',\n",
    "    bottom= False,\n",
    "    top= False,\n",
    "    labelbottom=False)\n",
    "plt.ylabel('percentage of explained variance')\n",
    "plt.xlabel('principal Components')\n",
    "plt.title('scree plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "851a0119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.5, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'C': [0.5, 1, 10, 100],\n",
    "        'gamma': ['scale',1,0.1,0.01,0.001,0.0001],\n",
    "        'kernel': ['rbf','linear']\n",
    "    },\n",
    "]\n",
    "\n",
    "optimal_params = GridSearchCV(\n",
    "    svm.SVC(),\n",
    "    param_grid,\n",
    "    cv=20,\n",
    "    scoring='accuracy',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "#print(len(all_features_scalled),len(y_train))\n",
    "all_features_scaled = sc().fit_transform(all_features)\n",
    "\n",
    "optimal_params.fit(all_features_scaled,all_labels)\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "80ec85f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, random_state=0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_rbf =  svm.SVC(random_state=0,C=0.5,gamma='scale',kernel='rbf')\n",
    "clf_svm_rbf.fit(all_features_scaled,all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c2f74",
   "metadata": {},
   "source": [
    "all_features_scalled=scale(all_features)\n",
    "clf_linear = svm.SVC(kernel='linear').fit(all_features_scalled[0:220,0:3].tolist(),all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1225ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 67. 118.  38.  29.  75.  49.  20.  27.  28.  24.  62.  25.  32.  19.\n",
      "  25.   4.  14.  59.  71.   4.  14.  33.  21.   9.  22.  15.  62.  36.\n",
      "  12.  15.  53.  26.  69.  17.   8.  36.  34.  18.  16.  21. 218.  15.\n",
      "   1.   8.  55.  26. 156.  24.  63.  52.  76.  33.  71.  54.  18.  56.\n",
      "  69.  49.  19.  15.] [-0.15381478  0.50338873 -0.15870501 -0.53880182 -0.05656006 -0.22078229\n",
      " -0.66314074 -0.59897412 -0.55636337 -0.49539981 -0.39745638 -0.00867534\n",
      " -0.50931693 -0.57140062 -0.51699316 -0.76605846 -0.73344441 -0.74935984\n",
      " -0.11686514 -0.70232453 -0.57016455 -0.37103049 -0.63435067 -0.66262609\n",
      " -0.45912989 -0.66141391 -0.27003142 -0.44887166 -0.75734749 -0.66655108\n",
      " -0.26227922 -0.56141743 -0.33503819 -0.67089342 -0.704344   -0.29828825\n",
      " -0.172011   -0.71305806 -0.6943609  -0.5509923   1.22088233 -0.65897583\n",
      " -0.72199487 -0.70036076 -0.26348987 -0.55021638  1.26106207 -0.61576646\n",
      " -0.45418584 -0.2122673   0.02329566 -0.48776939 -0.02881426 -0.37337079\n",
      " -0.66024637 -0.19616096 -0.00503825 -0.34173789 -0.58814978 -0.69387992]\n",
      "[54.9641375, 741.0903347144275, -0.1376370610488463] [ 1.65933918  0.06915058 -0.98774664]\n"
     ]
    }
   ],
   "source": [
    "Test_des_list=[]\n",
    "Test_hue_arr=[]\n",
    "for file in glob.glob(Test_data):    \n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kpts, des = sift.detectAndCompute(img_gray,None)\n",
    "    if len(kpts) < 1:\n",
    "        no_kpts = np.zeros((1, sift.descriptorSize()), np.float32)\n",
    "        Test_des_list.append((file, no_kpts))\n",
    "    else:\n",
    "        Test_des_list.append((file, des)) \n",
    "    h,s,v=cv2.split(img)\n",
    "    hue_mean=np.mean(h)  #calculate the mean of hue channel of each image\n",
    "    hue_var=np.var(h) \n",
    "    flatten_h = list(np.concatenate(h).flat)\n",
    "    hue_skewness=skew(flatten_h)\n",
    "    image_hue_info=[]\n",
    "    image_hue_info.append(hue_mean)\n",
    "    image_hue_info.append(hue_var)\n",
    "    image_hue_info.append(hue_skewness)\n",
    "    Test_hue_arr.append(image_hue_info) \n",
    "\n",
    "\n",
    "\n",
    "    # Stack all the descriptors vertically in a numpy array\n",
    "descriptors = Test_des_list[0][1]\n",
    "for file, descriptor in Test_des_list[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))\n",
    "\n",
    "\n",
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = descriptors.astype(float)\n",
    "\n",
    "k = 60  #k means awal 60 clusters\n",
    "voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Test_features = np.zeros((len(Test_hue_arr), k), \"float32\")\n",
    "for i in range(len(Test_hue_arr)):\n",
    "    words, distance = vq(Test_des_list[i][1],voc)\n",
    "    for w in words:\n",
    "        Test_features[i][w] += 1\n",
    "\n",
    "\n",
    "\n",
    "Test_features_scaled = scale(Test_features.tolist())\n",
    "print(Test_features[0],Test_features_scaled[0])\n",
    "\n",
    "Test_hue_arr_scaled = scale(Test_hue_arr)\n",
    "print(Test_hue_arr[0],Test_hue_arr_scaled[0])\n",
    "\n",
    "Test_all_features=np.concatenate((Test_hue_arr_scaled,Test_features_scaled.tolist()),axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ac54e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf ['Orange' 'Mango' 'Mango' 'Mango' 'Orange' 'Apple' 'Apple' 'Apple']\n"
     ]
    }
   ],
   "source": [
    "#predict the fruit type in a new image\n",
    "#print(\"LINEAR\",clf_linear.predict(Test_all_features))\n",
    "print(\"rbf\",clf_svm_rbf.predict(Test_all_features))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
