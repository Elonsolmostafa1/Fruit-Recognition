{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc27721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn import svm\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from skimage.filters import threshold_yen\n",
    "from skimage.exposure import rescale_intensity\n",
    "from scipy.stats import skew\n",
    "from itertools import chain\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler as sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117b05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apple_training_data='dataset\\\\Apples\\\\Train\\\\*.jpg'\n",
    "Orange_training_data='dataset\\\\Oranges\\\\Train\\\\*.jpg'\n",
    "Mango_training_data='dataset\\\\Mangoes\\\\Train\\\\*.jpg'\n",
    "Test_data='dataset\\\\test\\\\*.jpg'\n",
    "\n",
    "Apple_hue_arr = []   #Info of hue of  each apple pic\n",
    "Orange_hue_arr = []  #Info of hue of  each orange pic\n",
    "Mango_hue_arr = []   #Info of hue of  each mango pic\n",
    "Apple_des_list = []\n",
    "Orange_des_list = []\n",
    "Mango_des_list = []\n",
    "sift = cv2.SIFT_create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c0be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_glcm_features(gray_scale_img):\n",
    "    \"\"\"\n",
    "    Given a grayscale image with graylevels from 0 - 255, this function returns the contrast\n",
    "    and the homogeneity features of the image with the help of GLCM\n",
    "    \"\"\"\n",
    "    # Tip: Make sure you understand the input-output of everything you write, \n",
    "    # not doing that results in bugs that make you believe the lab is long\n",
    "    \n",
    "    #size of co-occ matrix = number of gray levels\n",
    "    image_array = np.array(gray_scale_img)\n",
    "    #print('first pixel= ', image_array[0][0])\n",
    "    coocurrence_matrix = greycomatrix(image_array, [1], [0])\n",
    "    contrast = greycoprops(coocurrence_matrix, 'contrast')\n",
    "    homogeneity = greycoprops(coocurrence_matrix, 'homogeneity')\n",
    "    #mean = greycoprops(coocurrence_matrix, 'mean')\n",
    "    energy = greycoprops(coocurrence_matrix, 'energy')\n",
    "    #entropy = greycoprops(coocurrence_matrix, 'entropy')\n",
    "    #variance = greycoprops(coocurrence_matrix, 'variance')\n",
    "    correlation = greycoprops(coocurrence_matrix, 'correlation')\n",
    "    return contrast, homogeneity, energy, correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9136bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(Apple_training_data):    \n",
    "    img = cv2.imread(file)  #read apple images\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)  #convert from RBG to HSV\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    contrast, homogeneity, energy, correlation= get_all_glcm_features((img_gray * 255).astype(np.uint8))\n",
    "    kpts, des = sift.detectAndCompute(img_gray,None)\n",
    "    if len(kpts) < 1:\n",
    "        no_kpts = np.zeros((1, sift.descriptorSize()), np.float32)\n",
    "        Apple_des_list.append((file, no_kpts))\n",
    "    else:\n",
    "        Apple_des_list.append((file, des)) \n",
    "    h,s,v=cv2.split(img)  #Split the the channels of each image\n",
    "    hue_mean=np.mean(h)  #calculate the mean of hue channel of each image\n",
    "    hue_var=np.var(h)    #calculate the var of hue channel of each image\n",
    "    flatten_h = list(np.concatenate(h).flat)  #convert the 2-D image to 1-D image\n",
    "    flatten_s = list(np.concatenate(s).flat)\n",
    "    hue_skewness=skew(flatten_h)   #calculate the skewness of hue channel of each image\n",
    "    image_hue_info=[]\n",
    "    image_hue_info.append(hue_mean)\n",
    "    image_hue_info.append(hue_var)\n",
    "    image_hue_info.append(hue_skewness)\n",
    "    #######################################################################\n",
    "    image_hue_info.append(contrast[0][0])\n",
    "    image_hue_info.append(homogeneity[0][0])\n",
    "    image_hue_info.append(energy[0][0])\n",
    "    image_hue_info.append(correlation[0][0])\n",
    "    #########################################################################\n",
    "    Apple_hue_arr.append(image_hue_info)\n",
    "print(len(Apple_hue_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f10fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the descriptors vertically in a numpy array\n",
    "descriptors = Apple_des_list[0][1]\n",
    "for file, descriptor in Apple_des_list[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6613cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = descriptors.astype(float)\n",
    "\n",
    "k = 60  #k means awal 60 clusters\n",
    "voc, variance = kmeans(descriptors_float, k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f14c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apple_features = np.zeros((72, k), \"float32\")\n",
    "for i in range(72):\n",
    "    words, distance = vq(Apple_des_list[i][1],voc)\n",
    "    for w in words:\n",
    "        Apple_features[i][w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791efcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apples_all_features=np.concatenate((Apple_hue_arr,Apple_features.tolist()),axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419b11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(Orange_training_data):    \n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    contrast, homogeneity, energy, correlation= get_all_glcm_features((img_gray * 255).astype(np.uint8))\n",
    "    kpts, des = sift.detectAndCompute(img_gray,None)\n",
    "    if len(kpts) < 1:\n",
    "        no_kpts = np.zeros((1, sift.descriptorSize()), np.float32)\n",
    "        Orange_des_list.append((file, no_kpts))\n",
    "    else:\n",
    "        Orange_des_list.append((file, des)) \n",
    "    h,s,v=cv2.split(img)\n",
    "    hue_mean=np.mean(h)  #calculate the mean of hue channel of each image\n",
    "    hue_var=np.var(h) \n",
    "    flatten_h = list(np.concatenate(h).flat)\n",
    "    hue_skewness=skew(flatten_h)\n",
    "    image_hue_info=[]\n",
    "    image_hue_info.append(hue_mean)\n",
    "    image_hue_info.append(hue_var)\n",
    "    image_hue_info.append(hue_skewness)\n",
    "    #######################################################################\n",
    "    image_hue_info.append(contrast[0][0])\n",
    "    image_hue_info.append(homogeneity[0][0])\n",
    "    image_hue_info.append(energy[0][0])\n",
    "    image_hue_info.append(correlation[0][0])\n",
    "    #########################################################################\n",
    "    Orange_hue_arr.append(image_hue_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491e337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the descriptors vertically in a numpy array\n",
    "descriptors = Orange_des_list[0][1]\n",
    "for file, descriptor in Orange_des_list[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e963b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = descriptors.astype(float)\n",
    "\n",
    "k = 60  #k means awal 60 clusters\n",
    "voc, variance = kmeans(descriptors_float, k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3dc6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "Orange_features = np.zeros((70, k), \"float32\")\n",
    "for i in range(70):\n",
    "    words, distance = vq(Orange_des_list[i][1],voc)\n",
    "    for w in words:\n",
    "        Orange_features[i][w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e56a7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "Oranges_all_features=np.concatenate((Orange_hue_arr,Orange_features.tolist()),axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12fba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(Mango_training_data):    \n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    contrast, homogeneity, energy, correlation= get_all_glcm_features((img_gray * 255).astype(np.uint8))\n",
    "    kpts, des = sift.detectAndCompute(img_gray,None)\n",
    "    if len(kpts) < 1:\n",
    "        no_kpts = np.zeros((1, sift.descriptorSize()), np.float32)\n",
    "        Mango_des_list.append((file, no_kpts))\n",
    "    else:\n",
    "        Mango_des_list.append((file, des)) \n",
    "    h,s,v=cv2.split(img)\n",
    "    hue_mean=np.mean(h)\n",
    "    hue_var=np.var(h)\n",
    "    flatten_h = list(np.concatenate(h).flat)\n",
    "    hue_skewness=skew(flatten_h)\n",
    "    image_hue_info=[]\n",
    "    image_hue_info.append(hue_mean)\n",
    "    image_hue_info.append(hue_var)\n",
    "    image_hue_info.append(hue_skewness)\n",
    "    #######################################################################\n",
    "    image_hue_info.append(contrast[0][0])\n",
    "    image_hue_info.append(homogeneity[0][0])\n",
    "    image_hue_info.append(energy[0][0])\n",
    "    image_hue_info.append(correlation[0][0])\n",
    "    #########################################################################\n",
    "    Mango_hue_arr.append(image_hue_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3551c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the descriptors vertically in a numpy array\n",
    "descriptors = Mango_des_list[0][1]\n",
    "for file, descriptor in Mango_des_list[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d45309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = descriptors.astype(float)\n",
    "\n",
    "k = 60  #k means awal 60 clusters\n",
    "voc, variance = kmeans(descriptors_float, k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bddbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Mango_features = np.zeros((78, k), \"float32\")\n",
    "for i in range(78):\n",
    "    words, distance = vq(Mango_des_list[i][1],voc)\n",
    "    for w in words:\n",
    "        Mango_features[i][w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae3a5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mangos_all_features=np.concatenate((Mango_hue_arr,Mango_features.tolist()),axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98f14307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "['Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Apple', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Orange', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango', 'Mango']\n",
      "220\n",
      "[48.96833125, 1190.4076741736064, -0.267091441509736, 90.96823707256047, 0.5237285380681689, 0.15935572676245768, 0.9856370567780426, 130.0, 112.0, 230.0, 159.0, 159.0, 84.0, 73.0, 101.0, 177.0, 126.0, 134.0, 145.0, 146.0, 96.0, 123.0, 40.0, 77.0, 125.0, 40.0, 89.0, 92.0, 59.0, 120.0, 155.0, 145.0, 97.0, 67.0, 100.0, 88.0, 180.0, 24.0, 143.0, 87.0, 157.0, 23.0, 97.0, 81.0, 146.0, 118.0, 106.0, 54.0, 168.0, 123.0, 149.0, 98.0, 133.0, 199.0, 81.0, 82.0, 193.0, 164.0, 211.0, 108.0, 113.0, 22.0, 26.0, 152.0, 102.0, 123.0, 135.0]\n"
     ]
    }
   ],
   "source": [
    "#print(len(Apple_hue_arr),len(Orange_hue_arr),len(Mango_hue_arr))\n",
    "a=[\"Apple\"]*len(Apples_all_features)\n",
    "o=[\"Orange\"]*len(Oranges_all_features)\n",
    "m=[\"Mango\"]*len(Mangos_all_features)\n",
    "all_labels=a+o+m\n",
    "print(len(all_labels))\n",
    "print(all_labels)\n",
    "\n",
    "all_features= Apples_all_features + Oranges_all_features + Mangos_all_features\n",
    "print(len(all_features))\n",
    "print(all_features[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff05e0",
   "metadata": {},
   "source": [
    "#pca\n",
    "pca = PCA()\n",
    "x_scaled_pca = pca.fit_transform(all_features)\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_*100,decimals=1)\n",
    "labels =[str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "\n",
    "plt.bar(x=range(1, len(per_var)+1), height=per_var)\n",
    "plt.tick_params(\n",
    "    axis='x',\n",
    "    which='both',\n",
    "    bottom= False,\n",
    "    top= False,\n",
    "    labelbottom=False)\n",
    "plt.ylabel('percentage of explained variance')\n",
    "plt.xlabel('principal Components')\n",
    "plt.title('scree plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "851a0119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'C': [0.5, 1, 10, 100],\n",
    "        'gamma': ['scale',1,0.1,0.01,0.001,0.0001],\n",
    "        'kernel': ['rbf']\n",
    "    },\n",
    "]\n",
    "\n",
    "optimal_params = GridSearchCV(\n",
    "    svm.SVC(),\n",
    "    param_grid,\n",
    "    cv=25,\n",
    "    scoring='accuracy',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "#print(len(all_features_scalled),len(y_train))\n",
    "#all_features_scaled = scale(all_features)\n",
    "\n",
    "optimal_params.fit(all_features,all_labels)\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80ec85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "clf_svm_rbf =  svm.SVC(random_state=0,C=0.5,gamma='scale',kernel='rbf')\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features,all_labels, test_size=0.1,random_state=0) \n",
    "\n",
    "#train the model\n",
    "clf_svm_rbf.fit(all_features,all_labels)\n",
    "# save the model to disk\n",
    "filename = 'frut_recognation_model.sav'\n",
    "pickle.dump(clf_svm_rbf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c2f74",
   "metadata": {},
   "source": [
    "all_features_scalled=scale(all_features)\n",
    "clf_linear = svm.SVC(kernel='linear').fit(all_features_scalled[0:220,0:3].tolist(),all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1225ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_des_list=[]\n",
    "Test_hue_arr=[]\n",
    "'''\n",
    "Apple_training_data='dataset\\\\Apples\\\\Test\\\\*.jpg'\n",
    "Orange_training_data='dataset\\\\Oranges\\\\Test\\\\*.jpg'\n",
    "Mango_training_data='dataset\\\\Mangoes\\\\Test\\\\*.jpg'\n",
    "'''\n",
    "for file in glob.glob(Test_data):    \n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    contrast, homogeneity, energy, correlation= get_all_glcm_features((img_gray * 255).astype(np.uint8))\n",
    "    kpts, des = sift.detectAndCompute(img_gray,None)\n",
    "    if len(kpts) < 1:\n",
    "        no_kpts = np.zeros((1, sift.descriptorSize()), np.float32)\n",
    "        Test_des_list.append((file, no_kpts))\n",
    "    else:\n",
    "        Test_des_list.append((file, des)) \n",
    "    h,s,v=cv2.split(img)\n",
    "    hue_mean=np.mean(h)  #calculate the mean of hue channel of each image\n",
    "    hue_var=np.var(h) \n",
    "    flatten_h = list(np.concatenate(h).flat)\n",
    "    hue_skewness=skew(flatten_h)\n",
    "    image_hue_info=[]\n",
    "    image_hue_info.append(hue_mean)\n",
    "    image_hue_info.append(hue_var)\n",
    "    image_hue_info.append(hue_skewness)\n",
    "    #######################################################################\n",
    "    image_hue_info.append(contrast[0][0])\n",
    "    image_hue_info.append(homogeneity[0][0])\n",
    "    image_hue_info.append(energy[0][0])\n",
    "    image_hue_info.append(correlation[0][0])\n",
    "    #########################################################################\n",
    "    Test_hue_arr.append(image_hue_info) \n",
    "\n",
    "\n",
    "\n",
    "    # Stack all the descriptors vertically in a numpy array\n",
    "descriptors = Test_des_list[0][1]\n",
    "for file, descriptor in Test_des_list[1:]:\n",
    "    descriptors = np.vstack((descriptors, descriptor))\n",
    "\n",
    "\n",
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = descriptors.astype(float)\n",
    "\n",
    "k = 60  #k means awal 60 clusters\n",
    "voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Test_features = np.zeros((len(Test_hue_arr), k), \"float32\")\n",
    "for i in range(len(Test_hue_arr)):\n",
    "    words, distance = vq(Test_des_list[i][1],voc)\n",
    "    for w in words:\n",
    "        Test_features[i][w] += 1\n",
    "\n",
    "\n",
    "\n",
    "#Test_features_scaled = scale(Test_features.tolist())\n",
    "#print(Test_features[0],Test_features_scaled[0])\n",
    "\n",
    "#Test_hue_arr_scaled = scale(Test_hue_arr)\n",
    "#print(Test_hue_arr[0],Test_hue_arr_scaled[0])\n",
    "\n",
    "Test_all_features=np.concatenate((Test_hue_arr,Test_features.tolist()),axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac54e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf ['Apple' 'Orange' 'Apple' 'Apple' 'Orange' 'Apple' 'Orange' 'Apple'\n",
      " 'Mango' 'Apple' 'Orange' 'Mango' 'Mango' 'Mango' 'Mango' 'Mango' 'Apple']\n"
     ]
    }
   ],
   "source": [
    "#predict the fruit type in a new image\n",
    "#print(\"LINEAR\",clf_linear.predict(Test_all_features))\n",
    "print(\"rbf\",clf_svm_rbf.predict(Test_all_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a837b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "#from sklearn import metrics\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
